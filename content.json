{"meta":{"title":"Hexo","subtitle":"","description":"","author":"shb","url":"http://yoursite.com","root":"/"},"pages":[{"title":"archives","date":"2020-03-12T10:10:31.000Z","updated":"2020-03-14T15:22:10.684Z","comments":false,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-12T10:10:07.000Z","updated":"2020-03-14T15:22:27.293Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-03-12T10:50:58.000Z","updated":"2020-03-12T10:51:30.051Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-03-14T15:21:19.000Z","updated":"2020-03-14T15:22:42.803Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-14T15:20:56.000Z","updated":"2020-03-14T15:20:56.086Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""}],"posts":[{"title":"Almeida–Pineda Recurrent Backpropagation","slug":"Recurrent-Propagation","date":"2020-09-21T02:15:32.000Z","updated":"2020-09-25T08:41:56.970Z","comments":true,"path":"2020/09/21/Recurrent-Propagation/","link":"","permalink":"http://yoursite.com/2020/09/21/Recurrent-Propagation/","excerpt":"","text":"由 Almeida 和 Pineda 分别提出的算法。 Model令 $(x,y)$ 为训练集中的一个数据点， 我们通过 $x$ 和 $y$ 建立一个动态系统。 我们定义能量函数 $E_{\\theta}(x,s)$ 和代价函数 $C_{\\theta}(y,s)$， 其中 $s$ 为隐藏状态， $\\theta$ 为我们希望训练的参数。 当 $x$ 和 $y$ 确定的时候，这两个函数的取值仅取决于 $s$ 和 $\\theta$。 简略起见， 我们把这两个函数里的 $x$ 和 $y$ 忽略，写作 $E_{\\theta}(s)$ 和 $C_{\\theta}(s)$。 这个动态系统的运行过程如下： 类似于梯度下降， $s$ 从初始状态开始， 沿着 $E_{\\theta}(s)$ 在 $s$ 处的梯度反方向下降，即： \\frac{\\mathrm{d}s}{\\mathrm{d}t} = -\\frac{\\partial E_{\\theta}}{\\partial s} (s) \\tag{1}这个过程称为松弛阶段(free phase)。 $s$ 最终会在 $E_\\theta$ 的作用下达到一个极小值点， 我们将其记作 $s_{\\theta}^{0}$， 称为松弛不动点。 注意到 $s$ 可能由于初值不同而收敛到不同的极值点，但简单起见，我们先不考虑这件事。 我们的目标是在给定 $x,y$ 的情况下，最小化松弛不动点处代价函数的值 $J(\\theta) := C_{\\theta}(s_{\\theta}^{0})$。 如果我们能够求出 $J$ 关于 $\\theta$ 的梯度， 那我们就可以用梯度下降的方式来优化 $\\theta$。 Method定义 $S_{\\theta}^{0}(s,t)$ 为隐状态 $s$ 根据方程 $(1)$ 转移到时间 $t$ 时得到的中间状态。这个状态在动态系统理论中被称为流(flow)。 我们根据$S_{\\theta}^{0}$的定义，来定义代价函数 $C_{\\theta}$ 的中间状态 $L_{\\theta}$： L_{\\theta}(s,t) := C_{\\theta}(S_{\\theta}^{0}(s,t)) \\tag{2}根据定义，我们显然有 $L_{\\theta}(s,0)=C_{\\theta}(s)$， 以及在 $t \\rightarrow +\\infty$ 时， $L_\\theta(s,t) \\rightarrow J(\\theta)$。 在 $E_{\\theta}$ 和 $C_{\\theta}$ 满足弱正则条件时，我们在 $t \\rightarrow +\\infty$ 时，有： \\frac{\\partial L_{\\theta}}{\\partial \\theta}(s,t) \\rightarrow \\frac{\\partial J}{\\partial \\theta}(\\theta) \\tag{3}定义： \\bar{S}_{t} := \\frac{\\partial L_{\\theta}}{\\partial s}(s_{\\theta}^0, t) \\tag{4} \\bar{\\Theta}_{t} := \\frac{\\partial L_{\\theta}}{\\partial \\theta}(s_{\\theta}^0, t) \\tag{5}注意两者的定义都建立在一个以松弛不动点为初状态的动态系统的基础上。两者分别是 $t$ 时刻代价函数在状态空间 $s$ 和参数空间 $\\theta$ 上的梯度。很显然，$\\bar{\\Theta}_{+ \\infty}$ 就是我们要求的 $\\frac{\\partial J}{\\partial \\theta}$。 Theorem 1 \\bar{S}_0 = \\frac{\\partial C_\\theta}{\\partial s}(s_\\theta^0) \\tag{6} \\bar{\\Theta}_0 = \\frac{\\partial C_\\theta}{\\partial \\theta}(s_\\theta^0) \\tag{7} \\frac{\\mathrm{d}\\bar{S}_t}{\\mathrm{d} t} = - \\frac{\\partial^{2} E_{\\theta}}{\\partial s^2}(s_\\theta^0) \\cdot \\bar{S}_t \\tag{8} \\frac{\\mathrm{d}\\bar{\\Theta}_t}{\\mathrm{d} t} = - \\frac{\\partial^{2} E_{\\theta}}{\\partial \\theta \\partial s}(s_\\theta^0) \\cdot \\bar{S}_t \\tag{9}Proof of Theorem 1$(6),(7)$ 都是显然的。 首先，我们证明，对于任意的$(s,t)$： \\frac{\\partial{L}}{\\partial{t}}(s,t) + \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s) = 0 \\tag{10}一个错误的证明：由方程$(1)$，我们可以得到，对于任意的$(s,t)$： \\frac{\\partial{L}}{\\partial{t}}(s,t) = \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\mathrm{d}s}{\\mathrm{d}t}(s) = \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot (-\\frac{\\partial{E}}{\\partial{s}}(s)) \\tag{11}事实上这里的第一个等号并不能直接由求导法则成立，而需要像后文中提取出单变量函数才能转化。 论文中的证明：根据定义，我们有： L(S^0(s,u),t)=L(s,t+u) \\tag{12}显然，右侧对 $u$ 和 $t$ 求导得到同样的结果，因此左边也应该得到同样的结果： \\begin{aligned} \\frac{\\partial{L}}{\\partial{t}}(S^0(s,u),t) &= \\frac{\\mathrm{d}L}{\\mathrm{d}u}(S^0(s,u),t) \\\\ &= \\frac{\\partial{L}}{\\partial{s}}(S^0(s,u),t) \\cdot \\frac{\\mathrm{d}s}{\\mathrm{d}u}(S^0(s,u)) \\\\ &= -\\frac{\\partial{L}}{\\partial{s}}(S^0(s,u),t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(S^0(s,u)) \\end{aligned}将 $u=0$ 代入，就能得到 $(10)$ 的结果。","categories":[],"tags":[]},{"title":"Linear Algebra","slug":"Algebra","date":"2020-04-04T09:20:13.000Z","updated":"2020-04-05T04:54:50.003Z","comments":true,"path":"2020/04/04/Algebra/","link":"","permalink":"http://yoursite.com/2020/04/04/Algebra/","excerpt":"","text":"Caylay-Hamilton theorem对于一个 $n \\times n$ 的矩阵 $A$，定义其特征多项式 p_A(\\lambda)=\\det(\\lambda I_n - A)$p_A(\\lambda)$ 可以写成 $\\displaystyle\\sum_{i=0}^{n}{c_i\\lambda^i}$ 的形式，注意这里的 $\\lambda$ 不一定是标量，可以是方阵。当 $\\lambda$ 是方阵时，$\\lambda^0=I_n$。 Caylay-Hamilton theorem 给出结果： p_A(A) = 0proof令 $g_A(\\lambda) = \\lambda I_n-A$, 则 $p_A=\\det(g_A)$。设 $B(\\lambda) = adj(g_A)$。 由于 $Xadj(X) = \\det(X)I_n$，我们有： B(\\lambda)g_A(\\lambda) = \\det(g_A(\\lambda))I_n = p_A(\\lambda)I_n \\tag{1}显然，矩阵 $B(\\lambda)$ 中每一项里 $\\lambda$ 的次数不超过 $n-1$。将 $B(\\lambda)$ 按 $\\lambda$ 的次数分拆，可以得到 B(\\lambda) = \\sum_{i=0}^{n-1}B_i\\lambda^i其中 $B_i$ 是 $n$ 阶方阵。 将其代入得： \\begin{aligned} B(\\lambda)g_A(\\lambda) &= B(\\lambda)(\\lambda I_n - A) \\\\ &= B_{n-1}\\lambda^n + \\sum_{i=1}^{n-1}{(B_{i-1}-B_iA)\\lambda^i}-B_0A \\end{aligned} \\tag{2}由于 $\\displaystyle p_A(\\lambda) = \\sum_{i=0}^{n}{c_i*\\lambda^i}$， 我们有： p_A(\\lambda)I_n = \\sum_{i=0}^{n}{c_i \\lambda^i I_n} \\tag{3}对比 $(2), (3)$ 中的系数，可以得到： \\left\\{ \\begin{aligned} B_{n-1} &= c_n \\lambda^n I_n \\\\ B_{n-2} - B_{n-1}A &= c_{n-1} \\lambda^{n-1} I_n \\\\ \\dotsc \\\\ B_{0} - B_{1}A &= c_{1} \\lambda I_n \\\\ -B_{0}A &= c_{0} I_n \\end{aligned} \\right. \\tag{4}将 $(4)$ 中等式从下往上依次乘以 $A_0, A_1, \\dotsc, A_n$, 相加即可得到 $p_A(a) = 0$。","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-03-12T10:06:23.416Z","updated":"2020-03-15T13:18:32.823Z","comments":true,"path":"2020/03/12/hello-world/","link":"","permalink":"http://yoursite.com/2020/03/12/hello-world/","excerpt":"","text":"hello, hello, hello","categories":[],"tags":[]}]}