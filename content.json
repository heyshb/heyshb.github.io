{"meta":{"title":"Hexo","subtitle":"","description":"","author":"shb","url":"http://yoursite.com","root":"/"},"pages":[{"title":"archives","date":"2020-03-12T10:10:31.000Z","updated":"2020-03-14T15:22:10.684Z","comments":false,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-12T10:10:07.000Z","updated":"2020-03-14T15:22:27.293Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-03-12T10:50:58.000Z","updated":"2020-03-12T10:51:30.051Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-03-14T15:21:19.000Z","updated":"2020-03-14T15:22:42.803Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-14T15:20:56.000Z","updated":"2020-03-14T15:20:56.086Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""}],"posts":[{"title":"The Graph Neural Network Model","slug":"gnn-0","date":"2020-10-09T03:37:37.000Z","updated":"2020-10-19T08:13:10.920Z","comments":true,"path":"2020/10/09/gnn-0/","link":"","permalink":"http://yoursite.com/2020/10/09/gnn-0/","excerpt":"","text":"这可能是比较早提出图神经网络模型的一篇文章，在这里摘录一些重点。 原文: The Graph Neural Network Model Abstract and Introduction很多关系型的数据需要用图论中的图结构来描述，例如分子模型、社交网络等。我们比较难用传统的神经网络来直接刻画这种结构关系，因此这篇文章提出了 Graph Neural Network 来扩展传统神经网络在图数据域的表达能力。具体来说，GNN 提供了一个映射函数 $\\tau(\\mathrm{G},n) \\in \\mathbb{R}^m$ 来把一个图 $G$ 以及其中的一个节点 $n$ 来嵌入到一个 $m$ 维的欧氏空间中。 在针对图整体(graph focused)的应用中， $\\tau$ 并不关心节点 $n$， 只是对图结构整体做一个分类或者回归任务， 例如对无向图描述的分子结构， 我们可能想设计一个网络来判断它是否对某种疾病具有疗效。 而在针对节点(node focused)的应用中， 模型依赖于每个节点的具体属性。 例如说， 在目标检测中， 可以通过 $\\tau$ 在区域邻接图上对节点进行标记， 进而通过标记结果选择检测出来的目标部分。 传统的机器学习方法在处理图数据的时候， 往往通过将图转化成更简单的表示， 如简单的向量， 但这个过程中可能会丢失拓扑结构之类的信息。 已有的一些方法通过预处理来处理无环图和一部分的有环图， 如 RNN、 马尔科夫链等。 这篇文章提出的方法扩展了已有方法， 得到了能比较好处理图结构的方法。 The Graph Neural Network ModelNotations$N$ 表示节点集合。 $E$ 表示边集合。 $ne[n]$ 表示 $n$ 的邻居集合。 $co[n]$ 表示 $n$ 的邻边集合。 节点和边的标签信息记为 $l_n$ 和 $l_{(n_1,n_2)}$， 维度分别为 $l_N$ 和 $l_E$。 令 $l$ 表示堆叠所有标签得到的结果。 我们将图分为 Positional Graphs 和 Non-positional Graphs。 Positional Graph 中， 对于节点 $n$ 的邻居是存有排列信息的。 例如在区域邻接图上， 可以有一个函数 $v_n$ 描述了 $n$ 邻居的逆时针顺序。 我们研究的问题定义在域 $\\mathcal{D} = \\mathcal{G} \\times \\mathcal{N}$ 上。 其中 $\\mathcal{G}$ 是一系列图的集合， 而 $\\mathcal{N}$ 是节点的集合。 一组训练集的形式如同 $\\mathcal{L} = \\left\\{ (G_i, n_{i,j}, t_{i,j}) | G_i = (N_i, E_i) \\in \\mathcal{G}; n_{i,j} \\in N_i; t_{i,j} \\in \\mathbb{R}^m\\right\\}$。 其中下标 $i,j$ 描述了第几个图的第几个有标注节点。 我们可以进一步把这些图视作一个大的不连通图的不同部分， 这样也可以写作 $\\mathcal{L} = (G, \\mathcal{T})$， 其中 $G$ 是整个大图而 $\\mathcal{T}$包含了所有被标注的节点-标签对 $(n_i, t_i)$。 The Model感性地来说， 可以认为图中的节点表示概念（或者实体）， 而边表示它们之间的关系。 一个概念应该由它自身的特征以及相关概念所定义。 我们对每个节点 $n$ 定义 $x_n \\in \\mathbb{R}^s$ 表示它的状态函数， 这由它自身的标签、邻居的标签、邻边标签、邻居的状态决定。即 $x_n = f_w(l_n, l_{co[n]}, l_{ne[n]}, x_{ne[n]})$。 其中 $f_w$ 被称为局部转化函数(local transition function)， 由参数 $w$ 控制。 在具体的任务中， 我们可能需要输出一些定制化的信息， 如节点分类的结果， 这通过另一个函数 $g_w$ 得到， $g_w$ 被称为局部输出函数(local output function)。 总之， 状态 $x$ 和输出 $o$ 如下定义： \\begin{align} x_n &= f_w(l_n, l_{co[n]}, l_{ne[n]}, x_{ne[n]}) \\\\ o_n &= g_w(l_n, x_n) \\end{align} \\tag{1}将所有的 $x_n$ 和 $o_n$ 合并到一个向量里，可以得到简化的形式。其中 $l_N$ 表示节点的标签。 $F_w$ 和 $G_w$ 被称为全局转化/输出函数。 \\begin{align} x &= F_w(x, l) \\\\ o &= G_w(x, l_N) \\end{align} \\tag{2}可以看到这个方程组中， 我们希望得到的 $x$ 是方程 $x = F_w(x, l)$ 的一个不动点。 由 Banach 不动点定理， 当 $F_w$ 是一个压缩映射的时候， 即存在 $0 &lt; \\mu &lt; 1$， 使得对于任意 $x, y$， 我们有 $\\parallel F_w(x, l) - F_w(y, l) \\parallel$， 那我们可以通过迭代来以指数级收敛的速度找到这个不动点。 稍后我们可以证明， 可以通过适当的实现来满足压缩映射的要求。 我们尝试用神经网络去实现 $f_w$ 和 $g_w$。 Learning Algorithm之前提到过， 训练集的形式是 $\\mathcal{L} = \\left\\{ (G_i, n_{i,j}, t_{i,j}) | G_i = (N_i, E_i) \\in \\mathcal{G}; n_{i,j} \\in N_i; t_{i,j} \\in \\mathbb{R}^m\\right\\}$。 对于 graph-focusd 的任务， 我们仅在图中选择一个点作为标注节点。 我们希望最小化代价函数： e_w = \\sum_{i=1}^{p} \\sum_{j=1}^{p_i}(t_{i,j} - \\varphi_w(G_i, n_{i,j})) \\tag{3}训练过程如下： 迭代 $T$ 轮近似得到 $x$ 的不动点 $x(T) \\approx x$。 计算 $\\frac{\\partial{e_w}}{\\partial{w}}$， 梯度下降优化参数。 Theorem 1: Differentiability在 $F_w$ 和 $G_w$ 可微， 且迭代求解不动点的动力系统稳态确定的情况下， 证明 $\\varphi(G,n)$ 也可微。 令 $\\Theta(x,w) = x - F_w(x,l)$， 由假设， $\\Theta$ 关于 $x$ 和 $w$ 均可微。 $\\frac{\\partial{\\Theta}}{\\partial{x}}(x, w) = I_{|s|} - \\frac{\\partial{F_w}}{\\partial{x}}(x, w)$。 由于 $F_w$ 是一个压缩映射， 存在一个 $0 \\le \\mu \\lt 1$ 使得 $\\parallel \\frac{\\partial{F_w}}{\\partial{x}}(x, w) \\parallel \\le \\mu$（考虑矩阵诱导范数的定义：最多能将一个向量的范数放缩多少倍， 再考虑压缩映射的定义， 是任意向量经过映射后的范数是原先的至多 $\\mu$ 倍， 而 Jacobi 矩阵定义了在邻域内的缩放比例， 则这个结论显然）， 原文说， 这说明 $\\parallel \\frac{\\partial{\\Theta}}{\\partial{x}}(x, w) \\parallel \\gt (1 - \\mu)$。 我觉得这里的符号可能有所混淆， 应该是一个类似于诱导范数的定义， 但是取的是放缩比例的最小值， 即 $\\min_{u \\not = 0}\\frac{\\parallel \\frac{\\partial{\\Theta}}{\\partial{x}}(x, w)u \\parallel}{\\parallel u \\parallel} \\gt (1 - \\mu)$。 这样就可以得出 $\\frac{\\partial{\\Theta}}{\\partial{x}}(x, w)$ 的行列式非 $0$ （否则直接取特征值 $0$ 对应的特征向量，就可以得到 $0$ 的放缩比例）。 根据隐函数定理， 和压缩映射的性质， 我们在任意的 $w$ 处有可微函数 $\\Psi(w)$ 使得 $\\Theta(\\Psi(w), w) = 0$， 即 $\\Psi(w) = F_w(\\Psi(w), l)$。 因此这个不动点是关于 $w$ 可微的。 而 $\\varphi(G, n)$ 不过是在外面再套了一层输出函数， 依然是可微的。 Theorem 2: Backpropagation \\frac{\\partial{e_w}}{\\partial{w}} = \\frac{\\partial{e_w}}{\\partial{o}} \\cdot \\frac{\\partial {G_w}}{\\partial{w}}(x, l_N) + z \\cdot \\frac{\\partial{F_w}}{\\partial{w}}(x,l) \\tag{4}其中， $z=\\lim_{t \\rightarrow -\\infty}z(t)$： z(t) = z(t+1) \\cdot \\frac{\\partial{F_w}}{\\partial{x}}(x,l) + \\frac{\\partial{e_w}}{\\partial{o}} \\cdot \\frac{\\partial{G_w}}{\\partial{x}}(x,l_N)","categories":[],"tags":[]},{"title":"Almeida–Pineda Recurrent Backpropagation","slug":"Recurrent-Propagation","date":"2020-09-21T02:15:32.000Z","updated":"2020-09-29T08:21:16.705Z","comments":true,"path":"2020/09/21/Recurrent-Propagation/","link":"","permalink":"http://yoursite.com/2020/09/21/Recurrent-Propagation/","excerpt":"","text":"由 Almeida 和 Pineda 分别提出的算法。 Model令 $(x,y)$ 为训练集中的一个数据点， 我们通过 $x$ 和 $y$ 建立一个动态系统。 我们定义能量函数 $E_{\\theta}(x,s)$ 和代价函数 $C_{\\theta}(y,s)$， 其中 $s$ 为隐藏状态， $\\theta$ 为我们希望训练的参数。 当 $x$ 和 $y$ 确定的时候，这两个函数的取值仅取决于 $s$ 和 $\\theta$。 简略起见， 我们把这两个函数里的 $x$ 和 $y$ 忽略，写作 $E_{\\theta}(s)$ 和 $C_{\\theta}(s)$。 这个动态系统的运行过程如下： 类似于梯度下降， $s$ 从初始状态开始， 沿着 $E_{\\theta}(s)$ 在 $s$ 处的梯度反方向下降，即： \\frac{\\mathrm{d}{s}}{\\mathrm{d}{t}} = -\\frac{\\partial E_{\\theta}}{\\partial s} (s) \\tag{1}这个过程称为松弛阶段(free phase)。 $s$ 最终会在 $E_\\theta$ 的作用下达到一个极小值点， 我们将其记作 $s_{\\theta}^{0}$， 称为松弛不动点。 注意到 $s$ 可能由于初值不同而收敛到不同的极值点，但简单起见，我们先不考虑这件事。 我们的目标是在给定 $x,y$ 的情况下，最小化松弛不动点处代价函数的值 $J(\\theta) := C_{\\theta}(s_{\\theta}^{0})$。 如果我们能够求出 $J$ 关于 $\\theta$ 的梯度， 那我们就可以用梯度下降的方式来优化 $\\theta$。 Method定义 $S_{\\theta}^{0}(s,t)$ 为隐状态 $s$ 根据方程 $(1)$ 转移到时间 $t$ 时得到的中间状态。这个状态在动态系统理论中被称为流(flow)。 我们根据$S_{\\theta}^{0}$的定义，来定义代价函数 $C_{\\theta}$ 的中间状态 $L_{\\theta}$： L_{\\theta}(s,t) := C_{\\theta}(S_{\\theta}^{0}(s,t)) \\tag{2}根据定义，我们显然有 $L_{\\theta}(s,0)=C_{\\theta}(s)$， 以及在 $t \\rightarrow +\\infty$ 时， $L_\\theta(s,t) \\rightarrow J(\\theta)$。 在 $E_{\\theta}$ 和 $C_{\\theta}$ 满足弱正则条件时，我们在 $t \\rightarrow +\\infty$ 时，有： \\frac{\\partial L_{\\theta}}{\\partial \\theta}(s,t) \\rightarrow \\frac{\\partial J}{\\partial \\theta}(\\theta) \\tag{3}定义： \\bar{S}_{t} := \\frac{\\partial L_{\\theta}}{\\partial s}(s_{\\theta}^0, t) \\tag{4} \\bar{\\Theta}_{t} := \\frac{\\partial L_{\\theta}}{\\partial \\theta}(s_{\\theta}^0, t) \\tag{5}注意两者的定义都建立在一个以松弛不动点为初状态的动态系统的基础上。两者分别是 $t$ 时刻代价函数在状态空间 $s$ 和参数空间 $\\theta$ 上的梯度。很显然，$\\bar{\\Theta}_{+ \\infty}$ 就是我们要求的 $\\frac{\\partial J}{\\partial \\theta}$。 Theorem 1 \\bar{S}_0 = \\frac{\\partial C_\\theta}{\\partial s}(s_\\theta^0) \\tag{6} \\bar{\\Theta}_0 = \\frac{\\partial C_\\theta}{\\partial \\theta}(s_\\theta^0) \\tag{7} \\frac{\\mathrm{d}\\bar{S}_t}{\\mathrm{d} {t}} = - \\frac{\\partial^{2} E_{\\theta}}{\\partial s^2}(s_\\theta^0) \\cdot \\bar{S}_t \\tag{8} \\frac{\\mathrm{d}\\bar{\\Theta}_t}{\\mathrm{d} {t}} = - \\frac{\\partial^{2} E_{\\theta}}{\\partial \\theta \\partial s}(s_\\theta^0) \\cdot \\bar{S}_t \\tag{9}Proof of Theorem 1为了书写方便，下面省略一部分的 $\\theta$。$(6),(7)$ 都是显然的。 首先，我们证明，对于任意的$(s,t)$： \\frac{\\partial{L}}{\\partial{t}}(s,t) + \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s) = 0 \\tag{10}一个错误的证明：由方程$(1)$，我们可以得到，对于任意的$(s,t)$： \\frac{\\partial{L}}{\\partial{t}}(s,t) = \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\mathrm{d}s}{\\mathrm{d}t}(s) = \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot (-\\frac{\\partial{E}}{\\partial{s}}(s)) \\tag{11}事实上这里的第一个等号并不能直接由求导法则成立，而需要像后文中提取出单变量函数才能转化。 论文中的证明：根据定义，我们有： L(S^0(s,u),t)=L(s,t+u) \\tag{12}显然，右侧对 $u$ 和 $t$ 求导得到同样的结果，因此左边也应该得到同样的结果： \\begin{aligned} \\frac{\\partial{L}}{\\partial{t}}(S^0(s,u),t) &= \\frac{\\mathrm{d}L}{\\mathrm{d}u}(S^0(s,u),t) \\\\ &= \\frac{\\partial{L}}{\\partial{s}}(S^0(s,u),t) \\cdot \\frac{\\mathrm{d}s}{\\mathrm{d}u}(S^0(s,u)) \\\\ &= -\\frac{\\partial{L}}{\\partial{s}}(S^0(s,u),t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(S^0(s,u)) \\end{aligned} \\tag{13}将 $u=0$ 代入 $(13)$，就能得到 $(10)$ 的结果。 将 $(10)$ 两边对 $s$ 求导，可以得到： \\frac{\\partial^2{L}}{\\partial{t}\\partial{s}}(s,t) + \\frac{\\partial^2(L)}{\\partial{s^2}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s) + \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial^2{E}}{\\partial{s^2}}(s)= 0 \\tag{14}把 $s = s_\\theta^0$ 代入上式，由于不动点处有 $\\frac{\\partial{E}}{\\partial s} (s_\\theta^0) = 0$，因此可以得到： \\frac{\\mathrm{d}}{\\mathrm{d}{t}}\\frac{\\partial{L}}{\\partial{s}}(s_\\theta^0,t) = -\\frac{\\partial^2{E}}{\\partial{s^2}}(s_\\theta^0) \\cdot \\frac{\\partial{L}}{\\partial{s}}(s_\\theta^0,t) \\tag{15}将 $\\bar{S}_{t} = \\frac{\\partial L_{\\theta}}{\\partial s}(s_{\\theta}^0, t)$ 代入，就可以得到方程 $(8)$。可以看到 $(6),(8)$ 刻画了一个关于 $\\bar{S}_t$ 的动态系统， 由于 $-\\frac{\\partial^2{E}}{\\partial{s^2}}(s_\\theta^0)$ 是定值，我们可以通过数值方法得到任意时刻 $\\bar{S}_t$ 的值。 这里一开始理解的时候出了一点问题。由于 $s_\\theta^0$ 是松弛不动点，所以 $L(s_\\theta^0,t)$ 在 $t$ 改变时是一个定值，所以我误认为左边恒等于 $0$。 但 $s_\\theta^0$ 的邻域并不是不动点，因此两个 $s$ 关于 $t$ 的导数是不同的。 类似之前的操作，我们将 $(10)$ 两边对 $\\theta$ 求导，可以得到： \\frac{\\partial^2{L}}{\\partial{t}\\partial{\\theta}}(s,t) + \\frac{\\partial^2{L}}{\\partial{s}\\partial{\\theta}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s) + \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial^2{E}}{\\partial{s}\\partial{\\theta}}(s) = 0同样将 $s = s_\\theta^0$ 代入，消去带有 $\\frac{\\partial{E}}{\\partial s} (s)$ 的项，就可以得到： \\frac{\\mathrm{d}}{\\mathrm{d}{t}}\\frac{\\partial{L}}{\\partial{\\theta}}(s_\\theta^0,t) = - \\frac{\\partial{L}}{\\partial{s}}(s_\\theta^0,t) \\cdot \\frac{\\partial^2{E}}{\\partial{s}\\partial{\\theta}}(s_\\theta^0) \\tag{16}即方程 $(9)$。 回过头看整个证明，重点在于方程 $10$ 把时域的导数和状态域的导数联系在了一起。在项 $\\frac{\\partial{L}}{\\partial{t}}(s,t)$ 对某个参数求导时，可以得到对应参数的导数在时域上的导数。 而$\\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s)$ 在松弛不动点处求导时，只会留下 $\\bar{S}_t$ 和某个与 $t$ 无关的二阶导数乘积，进而可以直接计算。 Algorithm 从任意初始状态 $s$ 出发，用方程 $(1)$ 迭代得到松弛不动点 $s_\\theta^0$。 根据 $(6)(7)(8)(9)$，得到 $\\bar{\\Theta}_{+ \\infty}$，即所求梯度。 Q&amp;AQ: 为什么 $\\bar{S}_t$ 一定可以收敛，也就是 $(8)$ 的左侧为什么一定可以迭代到 $0$？ A: 由于 $s_\\theta^0$ 是 $E_\\theta$ 的极小值点，所以此处的 Hessian 矩阵 $\\frac{\\partial^2{E_\\theta}}{\\partial{s^2}}(s_\\theta^0)$ 是正定的。令 $H = \\frac{\\partial^2{E_\\theta}}{\\partial{s^2}}(s_\\theta^0)$，则我们有： \\frac{\\mathrm{d}\\bar{S}_t}{\\mathrm{d} {t}} = -H \\cdot \\bar{S}_t 对于任意的初始$\\bar{S}_t$，我们将其投影到 $H$ 的特征向量上。由于 $H$ 是正定的，每个特征向量对应的特征值都是正的，且这些特征向量彼此正交。因此 $-H \\cdot \\bar{S}_t$ 在每个特征向量上的投影都和 $\\bar{S}_t$ 的对应投影反向，即每个分量上的投影 $p_i$ 都满足 $\\frac{\\mathrm{d}{p_i}}{\\mathrm{d}{t}} = -\\lambda_i \\cdot {p_i}$。这个微分方程的解是 $p_i = e^{\\ln(p_{i}^{0}) - \\lambda_{i} t}$，其中 $p_i^0$ 是该方向的投影初值。可以看出每个分量上都随时间指数收敛到 $0$，因此最终 $\\bar{S}_t$ 及其导数都会收敛到 $0$。回过头看 $\\bar{S}_t$ 的定义，在 $t \\rightarrow +\\infty$ 时，初状态 $s$ 的抖动最终都会在 $E_\\theta$ 的作用下修正（到达 $s_\\theta^0$），即$\\bar{S}_{+\\infty} = 0$，因此这个结果是符合直觉的。","categories":[],"tags":[]},{"title":"Linear Algebra","slug":"Algebra","date":"2020-04-04T09:20:13.000Z","updated":"2020-04-05T04:54:50.003Z","comments":true,"path":"2020/04/04/Algebra/","link":"","permalink":"http://yoursite.com/2020/04/04/Algebra/","excerpt":"","text":"Caylay-Hamilton theorem对于一个 $n \\times n$ 的矩阵 $A$，定义其特征多项式 p_A(\\lambda)=\\det(\\lambda I_n - A)$p_A(\\lambda)$ 可以写成 $\\displaystyle\\sum_{i=0}^{n}{c_i\\lambda^i}$ 的形式，注意这里的 $\\lambda$ 不一定是标量，可以是方阵。当 $\\lambda$ 是方阵时，$\\lambda^0=I_n$。 Caylay-Hamilton theorem 给出结果： p_A(A) = 0proof令 $g_A(\\lambda) = \\lambda I_n-A$, 则 $p_A=\\det(g_A)$。设 $B(\\lambda) = adj(g_A)$。 由于 $Xadj(X) = \\det(X)I_n$，我们有： B(\\lambda)g_A(\\lambda) = \\det(g_A(\\lambda))I_n = p_A(\\lambda)I_n \\tag{1}显然，矩阵 $B(\\lambda)$ 中每一项里 $\\lambda$ 的次数不超过 $n-1$。将 $B(\\lambda)$ 按 $\\lambda$ 的次数分拆，可以得到 B(\\lambda) = \\sum_{i=0}^{n-1}B_i\\lambda^i其中 $B_i$ 是 $n$ 阶方阵。 将其代入得： \\begin{aligned} B(\\lambda)g_A(\\lambda) &= B(\\lambda)(\\lambda I_n - A) \\\\ &= B_{n-1}\\lambda^n + \\sum_{i=1}^{n-1}{(B_{i-1}-B_iA)\\lambda^i}-B_0A \\end{aligned} \\tag{2}由于 $\\displaystyle p_A(\\lambda) = \\sum_{i=0}^{n}{c_i*\\lambda^i}$， 我们有： p_A(\\lambda)I_n = \\sum_{i=0}^{n}{c_i \\lambda^i I_n} \\tag{3}对比 $(2), (3)$ 中的系数，可以得到： \\left\\{ \\begin{aligned} B_{n-1} &= c_n \\lambda^n I_n \\\\ B_{n-2} - B_{n-1}A &= c_{n-1} \\lambda^{n-1} I_n \\\\ \\dotsc \\\\ B_{0} - B_{1}A &= c_{1} \\lambda I_n \\\\ -B_{0}A &= c_{0} I_n \\end{aligned} \\right. \\tag{4}将 $(4)$ 中等式从下往上依次乘以 $A_0, A_1, \\dotsc, A_n$, 相加即可得到 $p_A(a) = 0$。","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-03-12T10:06:23.416Z","updated":"2020-03-15T13:18:32.823Z","comments":true,"path":"2020/03/12/hello-world/","link":"","permalink":"http://yoursite.com/2020/03/12/hello-world/","excerpt":"","text":"hello, hello, hello","categories":[],"tags":[]}]}