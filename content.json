{"meta":{"title":"Hexo","subtitle":"","description":"","author":"shb","url":"http://yoursite.com","root":"/"},"pages":[{"title":"archives","date":"2020-03-12T10:10:31.000Z","updated":"2020-03-14T15:22:10.684Z","comments":false,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-12T10:10:07.000Z","updated":"2020-03-14T15:22:27.293Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-03-12T10:50:58.000Z","updated":"2020-03-12T10:51:30.051Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-03-14T15:21:19.000Z","updated":"2020-03-14T15:22:42.803Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-03-14T15:20:56.000Z","updated":"2020-03-14T15:20:56.086Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""}],"posts":[{"title":"Almeida–Pineda Recurrent Backpropagation","slug":"Recurrent-Propagation","date":"2020-09-21T02:15:32.000Z","updated":"2020-09-28T10:47:07.555Z","comments":true,"path":"2020/09/21/Recurrent-Propagation/","link":"","permalink":"http://yoursite.com/2020/09/21/Recurrent-Propagation/","excerpt":"","text":"由 Almeida 和 Pineda 分别提出的算法。 Model令 $(x,y)$ 为训练集中的一个数据点， 我们通过 $x$ 和 $y$ 建立一个动态系统。 我们定义能量函数 $E_{\\theta}(x,s)$ 和代价函数 $C_{\\theta}(y,s)$， 其中 $s$ 为隐藏状态， $\\theta$ 为我们希望训练的参数。 当 $x$ 和 $y$ 确定的时候，这两个函数的取值仅取决于 $s$ 和 $\\theta$。 简略起见， 我们把这两个函数里的 $x$ 和 $y$ 忽略，写作 $E_{\\theta}(s)$ 和 $C_{\\theta}(s)$。 这个动态系统的运行过程如下： 类似于梯度下降， $s$ 从初始状态开始， 沿着 $E_{\\theta}(s)$ 在 $s$ 处的梯度反方向下降，即： \\frac{\\mathrm{d}{s}}{\\mathrm{d}{t}} = -\\frac{\\partial E_{\\theta}}{\\partial s} (s) \\tag{1}这个过程称为松弛阶段(free phase)。 $s$ 最终会在 $E_\\theta$ 的作用下达到一个极小值点， 我们将其记作 $s_{\\theta}^{0}$， 称为松弛不动点。 注意到 $s$ 可能由于初值不同而收敛到不同的极值点，但简单起见，我们先不考虑这件事。 我们的目标是在给定 $x,y$ 的情况下，最小化松弛不动点处代价函数的值 $J(\\theta) := C_{\\theta}(s_{\\theta}^{0})$。 如果我们能够求出 $J$ 关于 $\\theta$ 的梯度， 那我们就可以用梯度下降的方式来优化 $\\theta$。 Method定义 $S_{\\theta}^{0}(s,t)$ 为隐状态 $s$ 根据方程 $(1)$ 转移到时间 $t$ 时得到的中间状态。这个状态在动态系统理论中被称为流(flow)。 我们根据$S_{\\theta}^{0}$的定义，来定义代价函数 $C_{\\theta}$ 的中间状态 $L_{\\theta}$： L_{\\theta}(s,t) := C_{\\theta}(S_{\\theta}^{0}(s,t)) \\tag{2}根据定义，我们显然有 $L_{\\theta}(s,0)=C_{\\theta}(s)$， 以及在 $t \\rightarrow +\\infty$ 时， $L_\\theta(s,t) \\rightarrow J(\\theta)$。 在 $E_{\\theta}$ 和 $C_{\\theta}$ 满足弱正则条件时，我们在 $t \\rightarrow +\\infty$ 时，有： \\frac{\\partial L_{\\theta}}{\\partial \\theta}(s,t) \\rightarrow \\frac{\\partial J}{\\partial \\theta}(\\theta) \\tag{3}定义： \\bar{S}_{t} := \\frac{\\partial L_{\\theta}}{\\partial s}(s_{\\theta}^0, t) \\tag{4} \\bar{\\Theta}_{t} := \\frac{\\partial L_{\\theta}}{\\partial \\theta}(s_{\\theta}^0, t) \\tag{5}注意两者的定义都建立在一个以松弛不动点为初状态的动态系统的基础上。两者分别是 $t$ 时刻代价函数在状态空间 $s$ 和参数空间 $\\theta$ 上的梯度。很显然，$\\bar{\\Theta}_{+ \\infty}$ 就是我们要求的 $\\frac{\\partial J}{\\partial \\theta}$。 Theorem 1 \\bar{S}_0 = \\frac{\\partial C_\\theta}{\\partial s}(s_\\theta^0) \\tag{6} \\bar{\\Theta}_0 = \\frac{\\partial C_\\theta}{\\partial \\theta}(s_\\theta^0) \\tag{7} \\frac{\\mathrm{d}\\bar{S}_t}{\\mathrm{d} {t}} = - \\frac{\\partial^{2} E_{\\theta}}{\\partial s^2}(s_\\theta^0) \\cdot \\bar{S}_t \\tag{8} \\frac{\\mathrm{d}\\bar{\\Theta}_t}{\\mathrm{d} {t}} = - \\frac{\\partial^{2} E_{\\theta}}{\\partial \\theta \\partial s}(s_\\theta^0) \\cdot \\bar{S}_t \\tag{9}Proof of Theorem 1为了书写方便，下面省略一部分的 $\\theta$。$(6),(7)$ 都是显然的。 首先，我们证明，对于任意的$(s,t)$： \\frac{\\partial{L}}{\\partial{t}}(s,t) + \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s) = 0 \\tag{10}一个错误的证明：由方程$(1)$，我们可以得到，对于任意的$(s,t)$： \\frac{\\partial{L}}{\\partial{t}}(s,t) = \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\mathrm{d}s}{\\mathrm{d}t}(s) = \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot (-\\frac{\\partial{E}}{\\partial{s}}(s)) \\tag{11}事实上这里的第一个等号并不能直接由求导法则成立，而需要像后文中提取出单变量函数才能转化。 论文中的证明：根据定义，我们有： L(S^0(s,u),t)=L(s,t+u) \\tag{12}显然，右侧对 $u$ 和 $t$ 求导得到同样的结果，因此左边也应该得到同样的结果： \\begin{aligned} \\frac{\\partial{L}}{\\partial{t}}(S^0(s,u),t) &= \\frac{\\mathrm{d}L}{\\mathrm{d}u}(S^0(s,u),t) \\\\ &= \\frac{\\partial{L}}{\\partial{s}}(S^0(s,u),t) \\cdot \\frac{\\mathrm{d}s}{\\mathrm{d}u}(S^0(s,u)) \\\\ &= -\\frac{\\partial{L}}{\\partial{s}}(S^0(s,u),t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(S^0(s,u)) \\end{aligned} \\tag{13}将 $u=0$ 代入 $(13)$，就能得到 $(10)$ 的结果。 将 $(10)$ 两边对 $s$ 求导，可以得到： \\frac{\\partial^2{L}}{\\partial{t}\\partial{s}}(s,t) + \\frac{\\partial^2(L)}{\\partial{s^2}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s) + \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial^2{E}}{\\partial{s^2}}(s)= 0 \\tag{14}把 $s = s_\\theta^0$ 代入上式，由于不动点处有 $\\frac{\\partial{E}}{\\partial s} (s_\\theta^0) = 0$，因此可以得到： \\frac{\\mathrm{d}}{\\mathrm{d}{t}}\\frac{\\partial{L}}{\\partial{s}}(s_\\theta^0,t) = -\\frac{\\partial^2{E}}{\\partial{s^2}}(s_\\theta^0) \\cdot \\frac{\\partial{L}}{\\partial{s}}(s_\\theta^0,t) \\tag{15}将 $\\bar{S}_{t} = \\frac{\\partial L_{\\theta}}{\\partial s}(s_{\\theta}^0, t)$ 代入，就可以得到方程 $(8)$。可以看到 $(6),(8)$ 刻画了一个关于 $\\bar{S}_t$ 的动态系统， 由于 $-\\frac{\\partial^2{E}}{\\partial{s^2}}(s_\\theta^0)$ 是定值，我们可以通过数值方法得到任意时刻 $\\bar{S}_t$ 的值。 这里一开始理解的时候出了一点问题。由于 $s_\\theta^0$ 是松弛不动点，所以 $L(s_\\theta^0,t)$ 在 $t$ 改变时是一个定值，所以我误认为左边恒等于 $0$。 但 $s_\\theta^0$ 的邻域并不是不动点，因此两个 $s$ 关于 $t$ 的导数是不同的。 类似之前的操作，我们将 $(10)$ 两边对 $\\theta$ 求导，可以得到： \\frac{\\partial^2{L}}{\\partial{t}\\partial{\\theta}}(s,t) + \\frac{\\partial^2{L}}{\\partial{s}\\partial{\\theta}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s) + \\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial^2{E}}{\\partial{s}\\partial{\\theta}}(s) = 0同样将 $s = s_\\theta^0$ 代入，消去带有 $\\frac{\\partial{E}}{\\partial s} (s)$ 的项，就可以得到： \\frac{\\mathrm{d}}{\\mathrm{d}{t}}\\frac{\\partial{L}}{\\partial{\\theta}}(s_\\theta^0,t) = - \\frac{\\partial{L}}{\\partial{s}}(s_\\theta^0,t) \\cdot \\frac{\\partial^2{E}}{\\partial{s}\\partial{\\theta}}(s_\\theta^0) \\tag{16}即方程 $(9)$。 回过头看整个证明，重点在于方程 $10$ 把时域的导数和状态域的导数联系在了一起。在项 $\\frac{\\partial{L}}{\\partial{t}}(s,t)$ 对某个参数求导时，可以得到对应参数的导数在时域上的导数。 而$\\frac{\\partial{L}}{\\partial{s}}(s,t) \\cdot \\frac{\\partial{E}}{\\partial{s}}(s)$ 在松弛不动点处求导时，只会留下 $\\bar{S}_t$ 和某个与 $t$ 无关的二阶导数乘积，进而可以直接计算。 Algorithm 从任意初始状态 $s$ 出发，用方程 $(1)$ 迭代得到松弛不动点 $s_\\theta^0$。 根据 $(6)(7)(8)(9)$，得到 $\\bar{\\Theta}_{+ \\infty}$，即所求梯度。 Q&amp;AQ: 为什么 $\\bar{S}_t$ 一定可以收敛，也就是 $(8)$ 的左侧为什么一定可以迭代到 $0$？ A: 由于 $s_\\theta^0$ 是 $E_\\theta$ 的极小值点，所以此处的 Hessian 矩阵 $\\frac{\\partial^2{E_\\theta}}{\\partial{s^2}}(s_\\theta^0)$ 是正定的。","categories":[],"tags":[]},{"title":"Linear Algebra","slug":"Algebra","date":"2020-04-04T09:20:13.000Z","updated":"2020-04-05T04:54:50.003Z","comments":true,"path":"2020/04/04/Algebra/","link":"","permalink":"http://yoursite.com/2020/04/04/Algebra/","excerpt":"","text":"Caylay-Hamilton theorem对于一个 $n \\times n$ 的矩阵 $A$，定义其特征多项式 p_A(\\lambda)=\\det(\\lambda I_n - A)$p_A(\\lambda)$ 可以写成 $\\displaystyle\\sum_{i=0}^{n}{c_i\\lambda^i}$ 的形式，注意这里的 $\\lambda$ 不一定是标量，可以是方阵。当 $\\lambda$ 是方阵时，$\\lambda^0=I_n$。 Caylay-Hamilton theorem 给出结果： p_A(A) = 0proof令 $g_A(\\lambda) = \\lambda I_n-A$, 则 $p_A=\\det(g_A)$。设 $B(\\lambda) = adj(g_A)$。 由于 $Xadj(X) = \\det(X)I_n$，我们有： B(\\lambda)g_A(\\lambda) = \\det(g_A(\\lambda))I_n = p_A(\\lambda)I_n \\tag{1}显然，矩阵 $B(\\lambda)$ 中每一项里 $\\lambda$ 的次数不超过 $n-1$。将 $B(\\lambda)$ 按 $\\lambda$ 的次数分拆，可以得到 B(\\lambda) = \\sum_{i=0}^{n-1}B_i\\lambda^i其中 $B_i$ 是 $n$ 阶方阵。 将其代入得： \\begin{aligned} B(\\lambda)g_A(\\lambda) &= B(\\lambda)(\\lambda I_n - A) \\\\ &= B_{n-1}\\lambda^n + \\sum_{i=1}^{n-1}{(B_{i-1}-B_iA)\\lambda^i}-B_0A \\end{aligned} \\tag{2}由于 $\\displaystyle p_A(\\lambda) = \\sum_{i=0}^{n}{c_i*\\lambda^i}$， 我们有： p_A(\\lambda)I_n = \\sum_{i=0}^{n}{c_i \\lambda^i I_n} \\tag{3}对比 $(2), (3)$ 中的系数，可以得到： \\left\\{ \\begin{aligned} B_{n-1} &= c_n \\lambda^n I_n \\\\ B_{n-2} - B_{n-1}A &= c_{n-1} \\lambda^{n-1} I_n \\\\ \\dotsc \\\\ B_{0} - B_{1}A &= c_{1} \\lambda I_n \\\\ -B_{0}A &= c_{0} I_n \\end{aligned} \\right. \\tag{4}将 $(4)$ 中等式从下往上依次乘以 $A_0, A_1, \\dotsc, A_n$, 相加即可得到 $p_A(a) = 0$。","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-03-12T10:06:23.416Z","updated":"2020-03-15T13:18:32.823Z","comments":true,"path":"2020/03/12/hello-world/","link":"","permalink":"http://yoursite.com/2020/03/12/hello-world/","excerpt":"","text":"hello, hello, hello","categories":[],"tags":[]}]}